# -*- coding: utf-8 -*-
"""Winton final 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l2HhTa5Z6x1ke177brzd5jMFcnX0SceW
"""



"""Imports"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Dense, GlobalMaxPool1D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD,Adam

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

from sklearn.base import BaseEstimator
from sklearn.svm import SVR
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import PowerTransformer
from sklearn.feature_selection import SelectKBest
from sklearn.pipeline import FeatureUnion
from IPython import get_ipython
from IPython.display import display, HTML

from sklearn.base import TransformerMixin
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GroupKFold
from sklearn.model_selection import GroupShuffleSplit
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import QuantileTransformer
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputRegressor
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.compose import TransformedTargetRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import LinearSVR

import os
import random
import pandas_profiling as pp
import cufflinks as cf
import seaborn as sns
import matplotlib.pyplot as plt

!pip install scikit-plot
import scikitplot as skplt
from yellowbrick.features import Rank2D
from collections import defaultdict

from progressbar import ProgressBar
import xgboost as xgb

from sklearn import ensemble
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error

from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout, LSTM, Masking
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from keras.callbacks import ModelCheckpoint

random.seed(1234)

train_df = pd.read_csv('/content/drive/MyDrive/winton/train.csv')
test_df= pd.read_csv('/content/drive/MyDrive/winton/test_2.csv')

"""Define a function and check missing value """

from tabulate import tabulate

def meta(train,test,missing_values = '未知',cols_ignore_missing = []):
    
    df = pd.concat([train,test]).reset_index(drop=True).fillna('未知')
    data = []
    for col in df.columns:
        # 定义role
        if col == 'target':
            role = 'target'
        elif col == 'id':
            role = 'id'
        else:
            role = 'feature'
        
        # 定义category
        if 'ind' in col:
            category = 'individual'
        elif 'car' in col:
            category = 'car'
        elif 'calc' in col:
            category = 'calculated'
        elif 'reg' in col:
            category = 'region'
        else:
            category = 'other'
        
        
        # 定义 level of measurements
        if 'bin' in col or col == 'target':
            level = 'binary'
        elif 'cat' in col[-3:] or col == 'id':
            level = 'nominal'
        elif df[col].dtype == 'float64' and df[col].replace(missing_values,np.nan).max()-df[col].replace(missing_values,np.nan).min() > 1:
            level = 'interval'
        elif df[col].dtype == 'float64' and df[col].replace(missing_values,np.nan).max()-df[col].replace(missing_values,np.nan).min() <= 1:
            level = 'ratio'
        elif df[col].dtype == 'int64':
            level = 'ordinal'
            
        # 定义 data type
        dtype = df[col].dtype
        
        # 定义 unique
        if col == 'id' or df[col].dtype == 'float64':
            uniq = 'Ignore'
        else:
            if col in cols_ignore_missing:
                uniq = df[col].nunique()
            else:
                uniq = df[col].replace({missing_values:np.nan}).nunique()
                
        # 定义 cardinality
        if uniq == 'Ignore':
            cardinality = 'Ignore'
        elif uniq <= 10:
            cardinality = 'Low Cardinality'
        elif uniq <= 30:
            cardinality = 'Medium Cardinality'
        else:
            cardinality = 'High Cardinality'
        
        # 定义 missing
        if col in cols_ignore_missing:
            missing = 0
        else:
            missing = sum(df[col] == missing_values)
            
        # 定义 missing percent
        missing_percent = f'{missing}({round(missing*100/len(df),2)}%)'
        
        # 定义 imputation
        if missing > df.shape[0]*0.4:
            imputation = 'remove'
        elif missing > 0:
            if level == 'binary' or level == 'nominal':
                imputation = ('mode')
            if level == 'ordinal':
                imputation = ('mode','median')
            if level == 'interval' or level == 'ratio':
                imputation = ('mode','median','mean')        
        else:
            imputation = "No Missing"
            
        # 定义 keep
        keep = True
        if col  == 'id' or imputation == 'remove':
            keep = False
        col_dict = {
            'colname': col,
            'role': role,
            'category': category,
            'level': level,
            'dtype': dtype,
            'cardinality': uniq,
            'cardinality_level':cardinality,
            'missing': missing,
            'missing_percent': missing_percent,
            'imputation':imputation,
            'keep': keep,
        }
        data.append(col_dict)
    meta = pd.DataFrame(data, columns=list(col_dict.keys()))
    meta.set_index('colname', inplace=True)
    
    return meta

def data_report(train,test,metadata,verbose = False):
    
    fullset = pd.concat([train,test]).reset_index(drop=True).fillna('未知')
    
    print(f"train总行数：{Fore.RED}{train.shape[0]}{Style.RESET_ALL} | test总行数：{Fore.BLUE}{test.shape[0]}{Style.RESET_ALL}")
    print(f"train总列数：{Fore.RED}{train.shape[1]}{Style.RESET_ALL} | test总列数：{Fore.BLUE}{test.shape[1]}{Style.RESET_ALL}")
    print(f"train总元素数：{train.size}")
    print(f"test总元素数：{test.size}")
    print('-'*50+ f"{Fore.RED}INFO{Style.RESET_ALL}"  + '-'*50)
    print('【train info】')
    train.info(verbose = verbose)
    print('-'*104)
    print('【test info】')
    test.info(verbose = verbose)
    
    if verbose:
    
        print('-'*48 + f"{Fore.RED}SUMMARY{Style.RESET_ALL}" + '-'*48)


        ############ SUMMARY #############
        print('*'*48 + f"{Fore.BLUE} COUNTS {Style.RESET_ALL}" + '*'*48)
        print('【Counts groupby role & level】'.upper())
        role_level_count = pd.DataFrame(
        {
            'count':metadata.groupby(['role','level']).size()
        }
        ).reset_index().sort_values(by = 'count',ascending=False)
        print(tabulate(role_level_count,tablefmt="grid",headers = ['role','level','count']))

        print('【Counts groupby role & category】'.upper())
        role_cate_count = pd.DataFrame(
        {
            'count':metadata.groupby(['role','category']).size()
        }
        ).reset_index().sort_values(by = 'count',ascending=False)
        print(tabulate(role_cate_count,tablefmt="grid",headers = ['role','category','count']))

        print('【Counts groupby role & cardinality_level】'.upper())
        role_cardinality_count = pd.DataFrame(
        {
            'count':metadata.groupby(['role','cardinality_level']).size()
        }
        ).reset_index().sort_values(by = 'count',ascending=False)
        print(tabulate(role_cardinality_count,tablefmt="grid",headers = ['role','cardinality_level','count']))


        print('*'*48 + f"{Fore.BLUE} MISSING {Style.RESET_ALL}" + '*'*48)
        print('【Cols to drop】'.upper())
        for col in metadata[metadata['keep'] == False].index:
            print(f" • {col}")

        print('【Cols to impute using (mode)】'.upper())
        for col in metadata[metadata['imputation'] == ('mode')].index:
            print(f" • {col}")

        print('【Cols to impute using (mode|median)】'.upper())
        for col in metadata[metadata['imputation'] == ('mode','median')].index:
            print(f" • {col}")

        print('【Cols to impute using (mode|median|mean)】'.upper())
        for col in metadata[metadata['imputation'] == ('mode','median','mean')].index:
            print(f" • {col}")

        print('*'*48 + f"{Fore.BLUE} CARDINALITY {Style.RESET_ALL}" + '*'*48)
        print('【Cols with medium cardinality】 ==> '.upper()+f'{Fore.YELLOW}PLEASE TAKE CARE OF USING ONEHOT-ENCODING{Style.RESET_ALL}')
        for col in metadata[metadata['cardinality_level'] == 'Medium Cardinality'].index:
            print(f" • {col}")

        print('【Cols with High cardinality】 ==> '.upper()+f'{Fore.YELLOW}PLEASE APPLY TARGET-ENCODING{Style.RESET_ALL}')
        for col in metadata[metadata['cardinality_level'] == 'High Cardinality'].index:
            print(f" • {Fore.GREEN}{col}{Style.RESET_ALL}")


        print('-'*42 + f"{Fore.RED}DESCRIPTIVE ANALYSIS{Style.RESET_ALL}" + '-'*42)
        conti_descrip = fullset[metadata[metadata['level'].isin(['interval','ratio'])].index].describe()
        print(tabulate(conti_descrip.T,tablefmt="grid",headers = conti_descrip.T.columns))

        print('-'*50 + f"{Fore.RED}META{Style.RESET_ALL}" + '-'*50)
        cols = ['role','category', 'level', 'dtype','cardinality', 'missing_percent','keep']
        print(tabulate(metadata[cols],tablefmt="grid",headers = cols))

"""Selects the feature part of our train and test data and check missing value"""

id = ['Id']
extracol = ['Ret_121','Ret_122','Ret_123','Ret_124','Ret_125','Ret_126','Ret_127','Ret_128','Ret_129','Ret_130',
'Ret_131','Ret_132','Ret_133','Ret_134','Ret_135','Ret_136','Ret_137','Ret_138','Ret_139','Ret_140','Ret_141',
'Ret_142','Ret_143','Ret_144','Ret_145','Ret_146','Ret_147','Ret_148','Ret_149','Ret_150','Ret_151','Ret_152',
'Ret_153','Ret_154','Ret_155','Ret_156','Ret_157','Ret_158','Ret_159','Ret_160','Ret_161','Ret_162','Ret_163',
'Ret_164','Ret_165','Ret_166','Ret_167','Ret_168','Ret_169','Ret_170','Ret_171','Ret_172','Ret_173','Ret_174',
'Ret_175','Ret_176','Ret_177','Ret_178','Ret_179','Ret_180','Ret_PlusOne','Ret_PlusTwo','Weight_Intraday','Weight_Daily']

train_feature_cols = [col for col in train_df.columns if col not in extracol] #meta data needs id
train_x=train_df[train_feature_cols]
test_feature_cols= [col for col in test_df.columns if col not in extracol] 
test_x=test_df[test_feature_cols]

metadata = meta(train_x,test_x)
missing_data = metadata[['missing','missing_percent','imputation']][metadata['missing']>0].sort_values(by = 'missing',ascending=False)
missing_data

"""based on the result, remove feature 1 and 10. define catagorical/nomial variable"""

cols_to_drop=['Feature_1','Feature_10']

#drop the cols here
train_df.drop(cols_to_drop,axis=1,inplace=True)
#drop the cols here
test_df.drop(cols_to_drop,axis=1,inplace=True)

targetcol = ['Ret_121','Ret_122','Ret_123','Ret_124','Ret_125','Ret_126','Ret_127','Ret_128','Ret_129','Ret_130',
'Ret_131','Ret_132','Ret_133','Ret_134','Ret_135','Ret_136','Ret_137','Ret_138','Ret_139','Ret_140','Ret_141',
'Ret_142','Ret_143','Ret_144','Ret_145','Ret_146','Ret_147','Ret_148','Ret_149','Ret_150','Ret_151','Ret_152',
'Ret_153','Ret_154','Ret_155','Ret_156','Ret_157','Ret_158','Ret_159','Ret_160','Ret_161','Ret_162','Ret_163',
'Ret_164','Ret_165','Ret_166','Ret_167','Ret_168','Ret_169','Ret_170','Ret_171','Ret_172','Ret_173','Ret_174',
'Ret_175','Ret_176','Ret_177','Ret_178','Ret_179','Ret_180','Ret_PlusOne','Ret_PlusTwo']

cat_features = ['Feature_5', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_12', 'Feature_13', 'Feature_16', 'Feature_20']

total=id+targetcol+cat_features

num_features=['Feature_2', 'Feature_3', 'Feature_4', 'Feature_6',
                      'Feature_11', 'Feature_14',
                      'Feature_17', 'Feature_18', 'Feature_19',
                      'Feature_21', 'Feature_22', 'Feature_23', 'Feature_24', 'Feature_25',
                      'Ret_MinusTwo', 'Ret_MinusOne']

num_cols = [col for col in test_df.columns if col not in total] 

feature_Minuteday_col=[col for col in num_cols if col not in num_features]

superextra=id+extracol+feature_Minuteday_col

"""fill missing value now"""

from sklearn.impute import SimpleImputer
mode_imputer = SimpleImputer(strategy='median')
mode_imputer=mode_imputer.fit(train_df[num_cols])
train_df[num_cols]=mode_imputer.transform(train_df[num_cols])
mode_imputer = SimpleImputer(strategy='median')
mode_imputer=mode_imputer.fit(test_df[num_cols])
test_df[num_cols]=mode_imputer.transform(test_df[num_cols])

from sklearn.impute import SimpleImputer 
mode_imputer = SimpleImputer( strategy='most_frequent')
mode_imputer=mode_imputer.fit(train_df[cat_features])
train_df[cat_features]=mode_imputer.transform(train_df[cat_features])
mode_imputer = SimpleImputer(strategy='most_frequent')
mode_imputer=mode_imputer.fit(test_df[cat_features])
test_df[cat_features]=mode_imputer.transform(test_df[cat_features])

quantile= QuantileTransformer(n_quantiles=300, output_distribution='normal', random_state=0)
train_df[num_cols]=quantile.fit_transform(train_df[num_cols])
quantile= QuantileTransformer(n_quantiles=300, output_distribution='normal', random_state=0)
test_df[num_cols]=quantile.fit_transform(test_df[num_cols])

trans= RobustScaler(quantile_range=[5, 95])
train_df[num_cols]=trans.fit_transform(train_df[num_cols])
trans= RobustScaler(quantile_range=[5, 95])
test_df[num_cols]=trans.fit_transform(test_df[num_cols])

train_df.to_csv('/content/drive/MyDrive/winton2/train_normal.csv')
test_df.to_csv('/content/drive/MyDrive/winton2/test_normal.csv')

"""LSTM for Minuteday prediction"""

#先检查train和test的col name
list(train_df.columns)

features = train_df.loc[:, 'Feature_2':'Feature_25'].values[:, None, :]

    # 120 return values
minuteday_return = train_df.loc[:, 'Ret_2':'Ret_120'].values[:, :, None]
    
features_repeated = np.repeat(features, minuteday_return.shape[1], axis=1)
total_minuteday_feature = np.dstack((features_repeated, minuteday_return))
target_minuteday = train_df.loc[:, 'Ret_121':'Ret_180']

def build_lstm_model(input_data, output_size, neurons=20, activ_func='relu',
                     dropout=.4, loss='mae', loss_weights=None, sample_weight_mode=None,
                     optimizer='adam'):
    model = Sequential()
    # https://stackoverflow.com/questions/49670832/keras-lstm-with-masking-layer-for-variable-length-inputs
    # https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN
    model.add(Masking(mask_value=0., input_shape=(input_data.shape[1], input_data.shape[2])))
    model.add(LSTM(neurons, input_shape=(input_data.shape[1], input_data.shape[2])))

    model.add(Dropout(dropout))
    model.add(Dense(units=2 * neurons))
    model.add(Dropout(dropout))
    model.add(Dense(units=output_size))
    model.add(Activation(activ_func))
    model.compile(loss=loss, loss_weights=loss_weights, sample_weight_mode=sample_weight_mode, optimizer=optimizer)
    print(model.summary())
    return model
  
lstm_neurons = 250
epochs = 3
batch_size = 500
loss = 'mae'
dropout = 0.20
optimizer = 'adam'
activ_func = 'relu'

checkpointer_intraday = ModelCheckpoint(filepath='best_weights_intraday.hdf5', 
                               verbose=1, save_best_only=True)
model_intraday = build_lstm_model(total_minuteday_feature, target_minuteday.shape[1], neurons=lstm_neurons, 
                               activ_func=activ_func, optimizer=optimizer,
                               dropout=dropout)
history_intraday = model_intraday.fit(total_minuteday_feature, target_minuteday, validation_split=0.25, batch_size=batch_size, 
                           verbose=1, shuffle=True,
                          epochs=epochs, callbacks=[checkpointer_intraday])

features = test_df.loc[:, 'Feature_2':'Feature_25'].values[:, None, :]

    # 120 return values
minuteday_return = test_df.loc[:, 'Ret_2':'Ret_120'].values[:, :, None]
    
features_repeated = np.repeat(features, minuteday_return.shape[1], axis=1)
test_total_minuteday_feature = np.dstack((features_repeated, minuteday_return))

pred_test_Y=model_intraday.predict(test_total_minuteday_feature)

unique, counts =np.unique(pred_test_Y, return_counts=True)
dict(zip(unique, counts))

pred_test_Y

"""Tried to upload with the minute prediction, results turned to be super noisy and increase the error significantly. Therefore, we choose not to predict minute anymore, only focus on the daily return.

We will reduce the feature dimension by creating an aggregated variable that combines all the 120 minute returns.
"""

#aggregate and standard deviation

train_sum = pd.DataFrame(columns=['minute_sum', 'minute_sd'])
test_sum = pd.DataFrame(columns=['minute_sum', 'minute_sd'])

train_sum['minute_sum'] = train_df[feature_Minuteday_col].sum(axis=1)
train_sum['minute_sd'] = train_df[feature_Minuteday_col].std(axis=1)
train_df = pd.concat([train_df, train_sum], axis=1)

test_sum['minute_sum'] = test_df[feature_Minuteday_col].sum(axis=1)
test_sum['minute_sd'] = test_df[feature_Minuteday_col].std(axis=1)
test_df = pd.concat([test_df, test_sum], axis=1)

num_features_final = ['Feature_2', 'Feature_3', 'Feature_4', 'Feature_6',
                      'Feature_11', 'Feature_14',
                      'Feature_17', 'Feature_18', 'Feature_19',
                      'Feature_21', 'Feature_22', 'Feature_23', 'Feature_24', 'Feature_25',
                      'Ret_MinusTwo', 'Ret_MinusOne', 'minute_sum', 'minute_sd']
features_final = num_features_final + cat_features   #待验证
dailytarget=['Ret_PlusOne', 'Ret_PlusTwo']

#change names!

num_transformer = Pipeline(steps=[
    ('norm', Normalizer(norm='l2'))
])

cat_transformer = Pipeline(steps=[
    #('pca', PCA(whiten=True, random_state=0)),
    ('bins', KBinsDiscretizer(n_bins=100, encode='onehot', strategy='quantile')),
    ('norm', Normalizer(norm='l2'))
])

# Combined preprocessing for numerical and categorical data
preprocessor_feature = ColumnTransformer(
    transformers=[
        ('num', num_transformer, num_features_final),        
        ('cat_nom', cat_transformer, cat_features)
])

preprocessor_target = Pipeline(steps=[
    ('quantile', QuantileTransformer(n_quantiles=300, output_distribution='normal', random_state=0))
])

# Define initial model
model = LinearSVR(epsilon=0.0, C=0.0005, loss='squared_epsilon_insensitive', random_state=0)  # 1727.860

# Define model pipeline for multi output regression
output = MultiOutputRegressor(model)
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor_feature), ('multioutput', output)])
estimator = TransformedTargetRegressor(regressor=model_pipeline, transformer=preprocessor_target)   #需要有这个


#     # Define grid parameters to search
grid_params = {
     'regressor__multioutput__estimator__C': [0.0001,0.0005, 0.001, 0.0015, 0.002,0.0025,0.003]
     }

scoring = 'neg_mean_absolute_error'

    # Define grid search specified scoring and cross-validation generator
print('Running grid searc CV...')
grid = GridSearchCV(estimator=estimator,   
                         param_grid=grid_params,
                         scoring=scoring)


    # Apply grid search and get parameters for best result
grid.fit(train_df[features_final], train_df[dailytarget])
best_params = grid.best_params_
best_estimator = grid.best_estimator_
score = -grid.best_score_

print(f'Best parameters = {grid.best_params_}')
print(f'Best MAE = {score}')

print('Done building model')

# Predict on test data
pred_test_Y = best_estimator.predict(test_df[features_final])

pred_test_Y

SAVE = True
#SAVE = False
if SAVE:
    # Create submission data
    ids = []
    preds = []
    for i, row in test_df.iterrows():
        for j in range(1, 61):
            ids.append(f'{i+1}_{j}')
            # OBS! We predict i_1 - i_60 as 0
            preds.append(0)
        ids.append(f'{i+1}_61')
        preds.append(pred_test_Y[i][0])  # D+1
        ids.append(f'{i+1}_62')
        preds.append(pred_test_Y[i][1])  # D+2

    submission_df = pd.DataFrame(
        list(zip(ids, preds)), columns=['Id', 'Predicted'])
    print(submission_df[(submission_df.Predicted != 0)].head(5))

submission_df.to_csv('/content/drive/MyDrive/winton2/submission1.csv', index=False)